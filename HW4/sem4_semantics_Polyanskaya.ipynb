{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать реализацию Word2vec в библиотеке **Gensim**, а в качестве предобученных моделей возьмем модели Андрея Кутузова и Лизы Кузьменко с сайта [RusVectōrēs.](https://rusvectores.org/ru/models/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей давайте возьмем \n",
    "\n",
    "1) araneum_none_fasttextcbow_300_5_2018 (fasttext) - модель, обученная на интернет-корпусе русского языка\n",
    "\n",
    "\n",
    "2) ruscorpora_upos_skipgram_300_5_2018 (word2vec) - модель, обученная НКРЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec + fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют несколько форматов, в которых могут храниться модели - .vec и .model \n",
    "\n",
    "1) Первый формат считается классическим вариантом модели word2vec. Для загрузки таакой модели надо воспользоваться методом *KeyedVectors.load_word2vec_format*. \n",
    "Модель может быть бинарной, для ее загрузки надо передать параметр binary, равный True. \n",
    "\n",
    "2) Формат .model - собственный формат gensim. Такую модель надо загружать с помощью метода *KeyedVectors.load*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1) если модель без тэгов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка модели\n",
    "\n",
    "model_file = 'araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model = KeyedVectors.load(model_file)\n",
    "\n",
    "\n",
    "#проверка наличия слова в словаре\n",
    "\n",
    "lemma = 'заграница'\n",
    "lemma in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.FastTextKeyedVectors at 0x21409d57550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение вектора документа\n",
    "\n",
    "Отлично, вектора для слов получены. Что с ними делать дальше? \n",
    "\n",
    "Есть два подхода (а точнее есть один, а второй мы придумали, потому что с одним жить нельзя).\n",
    "> Классика - для получения вектора документа нужно взять и усреднить все вектора его слов\n",
    " \n",
    "$$ vec_{doc} = \\frac {\\sum_{i=0}^{n} vec_i}{len(d)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# сделали препроцессинг, получили леммы \n",
    "lemmas = ['старинный', 'замок']\n",
    "\n",
    "# создаем вектор-маску\n",
    "lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "vec = np.zeros((model.vector_size,))\n",
    "\n",
    "# если слово есть в модели, берем его вектор\n",
    "for idx, lemma in enumerate(lemmas):\n",
    "    if lemma in model:\n",
    "        lemmas_vectors[idx] = model[lemma]\n",
    "        \n",
    "# проверка на случай, если на вход пришел пустой массив\n",
    "if lemmas_vectors.shape[0] is not 0:\n",
    "    vec = np.mean(lemmas_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Эксперимент - представим документ не в виде одного усредненного вектора, а как матрицу векторов входящих в него слов\n",
    "\n",
    "```\n",
    " слово1 |  v1_300\n",
    " слово2 |  v2_300\n",
    " слово3 |  v3_300\n",
    " слово4 |  v4_300\n",
    "```\n",
    "\n",
    "> Отлично, теперь каждый документ представлен в виде матрицы векторов своих слов. Но нам надо получить близость матрицы документа в коллекции и матрицы входящего запроса. Как? Умножим две матрицы друг на друга - одна матрица размером d x 300, другая q x 300 - получим попарную близость слов из каждого документа - матрицу размером d x q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем игрушечный пример кейса\n",
    "\n",
    "text1 = 'турция' \n",
    "text2 = 'нужна справка срочно'\n",
    "query = 'быстрая справка'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим матрицы всех документов\n",
    "\n",
    "def create_doc_matrix(text):\n",
    "    lemmas = text.split()\n",
    "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "    vec = np.zeros((model.vector_size,))\n",
    "\n",
    "    for idx, lemma in enumerate(lemmas):\n",
    "        if lemma in model:\n",
    "            lemmas_vectors[idx] = model[lemma]\n",
    "            \n",
    "    return lemmas_vectors    \n",
    "\n",
    "\n",
    "text1_m = create_doc_matrix(text1)\n",
    "text2_m = create_doc_matrix(text2)\n",
    "query_m = create_doc_matrix(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размер матрицы как и ожидали\n",
    "query_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02087945, 0.01183069]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на близость слов первого текста и слов запроса\n",
    "text1_m.dot(query_m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00173893,  0.0355643 ],\n",
       "       [ 0.00292079,  1.00000011],\n",
       "       [ 0.04900997,  0.33582122]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на близость слов второго текста и слов запроса\n",
    "text2_m.dot(query_m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032710136941469445, 1.0490100806379348]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_m = [text1_m, text2_m]\n",
    "\n",
    "    \n",
    "def search(docs, query, reduce_func=np.max, axis=0):\n",
    "    sims = []\n",
    "    for doc in docs:\n",
    "        sim = doc.dot(query.T)\n",
    "        sim = reduce_func(sim, axis=axis)\n",
    "        sims.append(sim.sum())\n",
    "    print(sims)\n",
    "    return np.argmax(sims)\n",
    "\n",
    "\n",
    "search(docs_m, query_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте поиск по нашему стандартному Covid корпусу с помощью модели на Araneum двумя способами:\n",
    "\n",
    "    1. преобразуйте каждый документ в вектор через усреднение векторов его слов и реализуйте поисковик как \n",
    "    обычно через умножение матрицы документов коллекции на вектор запроса \n",
    "    2. экспериментальный способ - реализуйте поиск ближайшего документа в коллекции к запросу, преобразовав \n",
    "    каждый документ в матрицу (количество слов x размер модели)\n",
    "    \n",
    "Посчитайте качество поиска для каждой модели на тех же данных, что и в предыдущем задании. В качестве препроцессинга используйте две версии - с удалением NER и без удаления.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## токенизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from razdel import tokenize, sentenize\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"russian\")\n",
    "stops.extend(['здравствовать', 'добрый', 'день', 'спасибо', 'пожалуйста'])\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def tokenizer(text_data):\n",
    "    tokens = [_.text for _ in list(tokenize(str(text_data).lower())) if not re.search('[^а-яА-ЯёЁa-zA-z]', _.text)]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def lemmatizer(tokens):\n",
    "    lem_text = []\n",
    "    for word in tokens.split(\" \"):\n",
    "        lem = morph.parse(word)[0].normal_form\n",
    "        if lem not in stops:\n",
    "            lem_text.append(lem)\n",
    "    return ' '.join(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_no_NE(text: str) -> str:\n",
    "    #text = text.replace('\\n', ' ')\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for span in doc.spans:\n",
    "        text = text.replace(span.text, \"\")\n",
    "    return lemmatizer(tokenizer(text)) #re.sub(r\"([^\\w\\s])\", r\" \\1 \", new_text) \n",
    "\n",
    "def preprocess_with_NE(text_data: str) -> str:\n",
    "    tokens = [_.text for _ in list(tokenize(str(text_data).lower())) if not re.search('[^а-яА-ЯёЁa-zA-z]', _.text)]\n",
    "    return lemmatizer(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c78c54202c4e4c830b721441c73de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2299), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9390862bccbc4d69bdd32bac3a66125c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2299), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries_data = pd.read_excel(\"queries_base.xlsx\").fillna('none')\n",
    "\n",
    "queries_data[\"with_NE\"] = queries_data[\"Текст вопроса\"].progress_apply(preprocess_with_NE)\n",
    "queries_data[\"no_NE\"] = queries_data[\"Текст вопроса\"].progress_apply(preprocess_no_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = queries_data[[\"Текст вопроса\", \"no_NE\", \"with_NE\", \"Номер связки\\n\"]][:int(len(queries_data)*0.7)]\n",
    "train = train.rename(columns = {\"Текст вопроса\": \"question\", \"Номер связки\\n\": \"index\"}, inplace = False)\n",
    "\n",
    "test = queries_data[[\"Текст вопроса\", \"no_NE\", \"with_NE\", \"Номер связки\\n\"]][int(len(queries_data)*0.7):]\n",
    "test = test.rename(columns = {\"Текст вопроса\": \"question\", \"Номер связки\\n\": \"index\"}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccc2afc4c74acaa24f31b8103d6a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers_data = pd.read_excel(\"answers_base.xlsx\")\n",
    "\n",
    "question_df = pd.DataFrame(columns=[\"index\", \"question\", \"no_NE\", \"with_NE\"])\n",
    "for question_chunk, answer_n in tqdm(answers_data[[\"Текст вопросов\", \"Номер связки\"]].values):\n",
    "    questions = question_chunk.split('\\n')\n",
    "    for q in questions:\n",
    "        question_df = question_df.append({\"index\": answer_n,\n",
    "                            \"question\": q,\n",
    "                            \"no_NE\": preprocess_no_NE(q),\n",
    "                            \"with_NE\": preprocess_with_NE(q)},\n",
    "                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.append(question_df, sort=False, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return v / np.sqrt(np.sum(v ** 2))\n",
    "\n",
    "def get_lemmas_vectors(lemm):\n",
    "    lemmas = lemm.split()\n",
    "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
    "\n",
    "    for idx, lemma in enumerate(lemmas):\n",
    "        lemmas_vectors[idx] = model[lemma]\n",
    "            \n",
    "    return lemmas_vectors\n",
    "\n",
    "def get_mean_vector(lemmas_vectors):\n",
    "    vec = np.zeros((model.vector_size,))\n",
    "    if lemmas_vectors.shape[0] is not 0:\n",
    "        vec = np.mean(lemmas_vectors, axis=0)\n",
    "        vec = normalize(vec)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8f34367fbb447997eaaa11dfa36b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2390), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a38dcd56c41f0a15ec7d503f85981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2390), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3afeb2557e947d4afad2d473247802e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2390), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c263935e805e4a319b8dc9f0571d6fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2390), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0d345ec481444191a77999b36d659d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcbfefc9cae469fb48e1432d44f484b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7781659ce7e64080b0d56f3b1c722341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d54870994b44fe6a9f045b49733c6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    df['vectors_no_NE'] = df['no_NE'].progress_apply(get_lemmas_vectors)\n",
    "    df['mean_vector_no_NE'] = df['vectors_no_NE'].progress_apply(get_mean_vector)\n",
    "    \n",
    "    df['vectors_with_NE'] = df['with_NE'].progress_apply(get_lemmas_vectors)\n",
    "    df['mean_vector_with_NE'] = df['vectors_with_NE'].progress_apply(get_mean_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## поиск\n",
    "### по векторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vector_no_NE(vec):\n",
    "    res = np.dot(np.vstack(train['mean_vector_no_NE'].values), vec)\n",
    "    index = np.argmax(res)\n",
    "    return train[\"index\"][index]\n",
    "\n",
    "def search_vector_with_NE(vec):\n",
    "    res = np.dot(np.vstack(train['mean_vector_with_NE'].values), vec)\n",
    "    index = np.argmax(res)\n",
    "    return train[\"index\"][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d4e3b9bd9c4c3c9723eea30a57e7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['pred_no_NE'] = test['mean_vector_no_NE'].progress_apply(search_vector_no_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6dc7e9f15c4b1db787e3351a887b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['pred_with_NE'] = test['mean_vector_with_NE'].progress_apply(search_vector_with_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159420289855072"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['pred_no_NE'] == test['index']).sum() / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5420289855072464"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['pred_with_NE'] == test['index']).sum() / len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### по матрицам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_matrix_no_NE(query):\n",
    "    sims = []\n",
    "    for doc in train['vectors_no_NE']:\n",
    "        sim = doc.dot(query.T)\n",
    "        if sim.shape[0] != 0:\n",
    "            sim = np.max(sim, axis=0)\n",
    "            sims.append(sim.sum())\n",
    "        else:\n",
    "            sims.append(0)\n",
    "    index = np.argmax(sims)\n",
    "    return train[\"index\"][index]\n",
    "\n",
    "def search_matrix_with_NE(query):\n",
    "    sims = []\n",
    "    for doc in train['vectors_with_NE']:\n",
    "        sim = doc.dot(query.T)\n",
    "        if sim.shape[0] != 0:\n",
    "            sim = np.max(sim, axis=0)\n",
    "            sims.append(sim.sum())\n",
    "        else:\n",
    "            sims.append(0)\n",
    "    index = np.argmax(sims)\n",
    "    return train[\"index\"][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736c73a3adef44b992c03a75d3c3600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['pred_matrix_no_NE'] = test['vectors_no_NE'].progress_apply(search_matrix_no_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260869565217391"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['pred_matrix_no_NE'] == test['index']).sum() / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c87c25310e4f19ae0521b6c7992046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['pred_matrix_with_NE'] = test['vectors_with_NE'].progress_apply(search_matrix_with_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4492753623188406"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test['pred_matrix_with_NE'] == test['index']).sum() / len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "||raw|natasha|lem|\n",
    "|--|------|-------|---|\n",
    "tf-idf|0.475362|0.45942|0.492754\n",
    "bm25|0.481159|0.514493|0.527536\n",
    "matrix||0.426087|0.449275\n",
    "vector||0.515942|0.542029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Анна Полянская, БКЛ171_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
